{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ce83fcb-a2bb-49e9-9fe6-038b7e7eab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time # Chong Ming. For Exercise task. To measure Inference Latency\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "with open(\"nebius_api_key\", \"r\") as file:\n",
    "    nebius_api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Nebius uses the same OpenAI() class, but with additional details\n",
    "nebius_client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b5c96ff-55e1-49f0-8df0-7d254a0e906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_string(text, max_line_length=80):\n",
    "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
    "\n",
    "    Args:\n",
    "        text: The string to print.\n",
    "        max_line_length: The maximum length of each line.\n",
    "    \"\"\"\n",
    "\n",
    "    output_lines = []\n",
    "    lines = text.split(\"\\n\") #Split the chunk of text retrieved from LLM into lines\n",
    "    for line in lines:       #Loop all the lines\n",
    "        current_line = \"\"\n",
    "        words = line.split() #Split the lines into words separate by whitespace\n",
    "        for word in words:\n",
    "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
    "                current_line += word + \" \"\n",
    "            else:\n",
    "                output_lines.append(current_line.strip())\n",
    "                current_line = word + \" \"\n",
    "        output_lines.append(current_line.strip())  # Append the last line\n",
    "    return \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb746206-50f8-41de-9bf6-19a5a42ba084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_llm(prompt: str,\n",
    "                    system_prompt,\n",
    "                    max_tokens=2056,\n",
    "                    client=nebius_client,\n",
    "                    model=\"\",\n",
    "                    prettify=False,\n",
    "                    temperature=0.7) -> str:\n",
    "\n",
    "    messages = []\n",
    "    #print(\"\\nModel Type in answer_with_llm: \"+model+\"\\n\")\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if prettify:\n",
    "        return prettify_string(completion.choices[0].message.content)\n",
    "    else:\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb2721da-208c-47d0-8929-31171a12f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- New BLOCK HERE for translation---\n",
    "class MMLUSample(BaseModel):\n",
    "    question: str\n",
    "    A: str\n",
    "    B: str\n",
    "    C: str\n",
    "    D: str\n",
    "    correct_answer: str\n",
    "\n",
    "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
    "    completion = nebius_client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",  # or your preferred model\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Translate this MMLU sample into {target_language}:\\n\n",
    "Question: {sample.question}\\n\n",
    "A: {sample.A}\\n\n",
    "B: {sample.B}\\n\n",
    "C: {sample.C}\\n\n",
    "D: {sample.D}\\n\n",
    "Correct answer: {sample.correct_answer}\\n\n",
    "Output the result as a JSON object with the same structure.\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        extra_body={\"guided_json\": MMLUSample.model_json_schema()}\n",
    "    )\n",
    "    translated = MMLUSample.model_validate_json(completion.choices[0].message.content)\n",
    "    translated.correct_answer = sample.correct_answer\n",
    "    return translated\n",
    "\n",
    "# --- END OF BLOCK ---\n",
    "\n",
    "'''\n",
    "This code builds an LLM evaluation tool using the MMLU dataset, a benchmark set of multiple-choice questions get from hugging face. The goal is to:\n",
    "Ask the model questions, Evaluate its answers, Measure how accurate the model is on a specific topic.\n",
    "This class runs the whole evaluation process.\n",
    "'''\n",
    "class MMLUEvaluator:\n",
    "    #Initializes the evaluator with:\n",
    "    #a system prompt (model’s role/instructions), a custom user prompt, a topic.\n",
    "    \n",
    "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
    "                 topic: str = \"high_school_mathematics\", language: str = \"English\"): #29Jun Add in new argument for Language\n",
    "        \"\"\"\n",
    "        Initialize the MMLU evaluator.\n",
    "\n",
    "        Args:\n",
    "            system_prompt: Optional system prompt for the model\n",
    "            prompt: Custom prompt for the model\n",
    "            topic: Which topic to choose\n",
    "        \"\"\"\n",
    "        \n",
    "        self.language = language #29Jun Add in this line for language\n",
    "        self.topic = topic\n",
    "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
    "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
    "\n",
    "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
    "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
    "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
    "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER OPTIONS:\n",
    "A: {A}\n",
    "B: {B}\n",
    "C: {C}\n",
    "D: {D}\n",
    "\"\"\"\n",
    "\n",
    "        self.questions, self.choices, self.answers, self.translated_q = self.load_mmlu_data(topic=self.topic)\n",
    "\n",
    "    '''\n",
    "    Add in load_mmlu_data function to translate the dataset\n",
    "    The function loads a dataset from the MMLU benchmark for a given topic\n",
    "    ''' \n",
    "\n",
    "    \n",
    "    def load_mmlu_data(self, topic: str):\n",
    "        \n",
    "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\") #Load Dataset from HuggingFace. \"cais/mmlu\" is the dataset repository name.\n",
    "        dataset = pd.DataFrame(dataset) #Converts the HuggingFace dataset object to a Pandas DataFrame \n",
    "\n",
    "        questions = dataset[\"question\"] #Extract qns Texts\n",
    "    \n",
    "        #Extract ans choices\n",
    "        #dataset[\"choices\"] contains answer choices as a list like [\"choice1\", \"choice2\", \"choice3\", \"choice4\"]\n",
    "        #.tolist() converts it from a Series of lists to a list of lists\n",
    "        #The result is turned into a new DataFrame with columns explicitly labeled \"A\", \"B\", \"C\", \"D\" to match multiple-choice options\n",
    "        choices = pd.DataFrame(dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]) #Extract ans choices\n",
    "\n",
    "        #Convert Numeric Answers to A/B/C/D\n",
    "        #The dataset gives the correct answer as integers (0, 1, 2, 3), where:0 → A, 1 → B, 2 → C, 3 → D\n",
    "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
    "\n",
    "        if self.language != \"English\":\n",
    "            #Initializes two empty lists to store translated questions and translated answer options.\n",
    "            #Loops over each row in the dataset to perform translations one by one.\n",
    "            translated_questions = []\n",
    "            translated_choices = []\n",
    "            for i in range(len(dataset)):\n",
    "                #Creates a structured MMLUSample object for a single row.\n",
    "                #This object is needed for guided LLM translation using pydantic.\n",
    "                sample = MMLUSample(\n",
    "                    question=questions[i],\n",
    "                    A=choices.iloc[i][\"A\"],\n",
    "                    B=choices.iloc[i][\"B\"],\n",
    "                    C=choices.iloc[i][\"C\"],\n",
    "                    D=choices.iloc[i][\"D\"],\n",
    "                    correct_answer=answers[i]\n",
    "                \n",
    "                    )\n",
    "                \n",
    "                translated = translate_mmlu_sample(sample, self.language)\n",
    "                translated_questions.append(translated.question)\n",
    "                #Store the Translated Results\n",
    "                translated_choices.append({\n",
    "                    \"A\": translated.A,\n",
    "                    \"B\": translated.B,\n",
    "                    \"C\": translated.C,\n",
    "                    \"D\": translated.D\n",
    "                    })\n",
    "            #After translation is complete, replace the original questions and choices with the translated ones.\n",
    "            questions = translated_questions\n",
    "            choices = pd.DataFrame(translated_choices)\n",
    "\n",
    "            #Return these 3 information for evaluation\n",
    "            #questions: a list or Series of questions (translated if needed)\n",
    "            #choices: a DataFrame with answer options labeled \"A\", \"B\", \"C\", \"D\"\n",
    "            #answers: a Series of correct answers (\"A\" to \"D\")\n",
    "            return questions, choices, answers, translated_questions\n",
    "        else:\n",
    "            # For English, just return original questions and choices\n",
    "            return questions, choices, answers, list(questions)  # last one is a dummy for translated_q\n",
    "\n",
    "\n",
    "    def extract_answer(self, solution: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the letter answer from model's response. Example: If the model writes #ANSWER: C, this grabs the C.\n",
    "\n",
    "        Args:\n",
    "            response: Raw model response\n",
    "\n",
    "        Returns:\n",
    "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
    "        \"\"\"\n",
    "        # Look for a single letter answer in the response\n",
    "        try:\n",
    "            #print(\"Print solution: \"+solution)\n",
    "            #answer = solution.split('#ANSWER:')[1].strip()\n",
    "        #except:\n",
    "         #   answer = \"Failed to parse\"\n",
    "        #return answer\n",
    "            \n",
    "            #the above codes has a flaw where if LLM response contains more than 1 \"#ANSWER:\" not matching the real answer, accuracy become zero.\n",
    "            #Modify the codes to look for #ANSWER: followed by optional spaces, then a capital letter A-D.\n",
    "            match = re.search(r\"#ANSWER:\\s*([A-D])\", solution)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return \"Failed to parse\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting answer: {e}\")\n",
    "            return \"Failed to parse\"\n",
    "\n",
    "\n",
    "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
    "                                 correct_answer: str,\n",
    "                                 client, model) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Evaluate a single question. Sends one question to the model.\n",
    "        Collects its answer compares it to the correct answer and return if:\n",
    "        Whether the answer was correct,\n",
    "        The chosen answer,\n",
    "        The model's full response.\n",
    "\n",
    "        Args:\n",
    "            question: Formatted question string\n",
    "            correct_answer: Correct answer letter\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (is_correct, extracted_answer, model_response)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()  # Chong Ming. For Exercise task. Add this line to measure inference latency. Start timer\n",
    "            model_response = answer_with_llm(\n",
    "                prompt=self.prompt.format(\n",
    "                    client=client, model=model,\n",
    "                    topic_prettified=self.topic_prettified,\n",
    "                    question=question,\n",
    "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
    "                ),\n",
    "                system_prompt=self.system_prompt,\n",
    "                model = model, #Put this in to insert in answer_with_llm function\n",
    "                prettify=False\n",
    "            )\n",
    "            # Chong Ming. For Exercise task. Elapsed time\n",
    "            end_time = time.time()  # End timer\n",
    "            inference_time = end_time - start_time\n",
    "            \n",
    "            answer = self.extract_answer(model_response)\n",
    "            is_correct = (answer.upper() == correct_answer.upper())\n",
    "            #return is_correct, answer, model_response\n",
    "\n",
    "            # Chong Ming. For Exercise task. Modify with additional inference_time for return\n",
    "            return is_correct, answer, model_response, inference_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating question: {e}\")\n",
    "            return False, None, None\n",
    "\n",
    "    def run_evaluation(self, client=nebius_client, model=\"none\",\n",
    "                       n_questions=50) -> Dict:\n",
    "        \"\"\"\n",
    "        Runs evaluation on the first n questions (default: 50)\n",
    "        Loops through each question, checks if the model’s answer is right\n",
    "        Calculates the accuracy (correct answers ÷ total)\n",
    "        Returns an accuracy score and all the answers/responses in a log\n",
    "\n",
    "        Args\n",
    "            client: Which client to use (OpenAI or Nebius)\n",
    "            model: Which model to use\n",
    "            n_questions: How many first questions to take\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        evaluation_log = []\n",
    "        correct_count = 0\n",
    "        inference_times = [] #Chong Ming. For Exercise task. to calculate average inference time, store inference_time in a list. Method 1\n",
    "        \n",
    "        total_inference_time = 0.0   #Chong Ming. For Exercise task. For purpose of collecting the time and derived an average. Method 2\n",
    "        \n",
    "        #print(\"The model in run_evaluation: \"+model)\n",
    "        #print(\"The model in n_questions: \"+str(n_questions))\n",
    "\n",
    "        if n_questions:\n",
    "            n_questions = min(n_questions, len(self.questions))\n",
    "        else:\n",
    "            n_questions = len(self.questions)\n",
    "\n",
    "        for i in tqdm(range(n_questions)):\n",
    "            is_correct, answer, model_response, inference_time = self.evaluate_single_question(\n",
    "                question=self.questions[i],\n",
    "                choices=self.choices.iloc[i],\n",
    "                correct_answer=self.answers[i],\n",
    "                client=client,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "                \n",
    "            evaluation_log.append({\n",
    "                'answer': answer,\n",
    "                'model_response': model_response,\n",
    "                'is_correct': is_correct,\n",
    "                'inference_time': inference_time #Chong Ming. For Exercise. Append only interence time for Method 1.\n",
    "            })\n",
    "        \n",
    "        #Chong Ming. For Exercise task. Adding up the time. Method 1\n",
    "        total_inference_time += inference_time \n",
    "        \n",
    "        #Chong Ming. For Exercise task. To keep adding into 'inference_times' list. Method 2    \n",
    "        inference_times.append(inference_time) \n",
    "\n",
    "        accuracy = correct_count / n_questions\n",
    "        avg_inference_time = total_inference_time / n_questions #Chong Ming. Calculate average. Method 1\n",
    "        avg_inference_time2 = sum(inference_times) / len(inference_times) if inference_times else 0. # Method 2\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'accuracy': accuracy,\n",
    "            'evaluation_log': evaluation_log,\n",
    "            'avg_inference_time': avg_inference_time, #Chong Ming. Add in to return additional info on the time results\n",
    "            'avg_inference_time_Method2': avg_inference_time2, #Chong Ming. Add in to return additional info on the time results in list\n",
    "        }\n",
    "\n",
    "        return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d38f946-2431-412d-9b32-9a1389d8c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:29<00:00,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(topic=\"global_facts\", language=\"English\")\n",
    "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                         n_questions=5)\n",
    "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d4c6a1e-3f59-44fb-bf29-31e397b96ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Log: [{'answer': 'C', 'model_response': \"To answer this question, I'll consider the global trend of obesity and overweight rates. According to various reports and studies from reputable organizations such as the World Health Organization (WHO), there has been a steady increase in overweight and obesity rates globally over the past few decades.\\n\\nAs of 2016, the WHO reported that approximately 39% of adults aged 18 years or older worldwide were overweight, and about 13% were obese. This suggests that the majority of adults were either overweight or obese, with a significant proportion being overweight.\\n\\nConsidering the options provided, I'll evaluate them as follows:\\n\\nA: 10% - This is an underestimation, given the reported global rates.\\n\\nB: 20% - This is still lower than the reported global average.\\n\\nC: 40% - This aligns closely with the reported global rate of overweight adults as of 2016.\\n\\nD: 80% - This is an overestimation, as the global average is not this high.\\n\\nBased on the available information and trends, I believe that the most accurate answer is:\\n\\n#ANSWER: C\", 'is_correct': True, 'inference_time': 4.063610553741455}, {'answer': 'C', 'model_response': 'To solve this, let\\'s break it down step by step:\\n\\n1. **Understanding GDP per capita in 1850**: We\\'re looking for the GDP per capita in the United States in 1850, adjusted for inflation and PPP (Purchasing Power Parity) in 2011 prices. This adjustment is crucial as it allows us to compare economic output across different time periods and currencies.\\n\\n2. **Considering the context of 1850**: The mid-19th century was a time of significant economic growth and industrialization in the United States, following the conclusion of the Civil War. The period saw the expansion of railroads, the growth of cities, and an increase in agricultural productivity.\\n\\n3. **Adjusting for inflation and PPP**: The key here is to understand what \"adjusting for inflation and PPP in 2011 prices\" means. This essentially means we\\'re comparing the purchasing power of the U.S. dollar in 1850 to its purchasing power in 2011. This adjustment helps account for the fact that a dollar in 2011 has more purchasing power than a dollar in 1850 due to inflation.\\n\\n4. **Estimating GDP per capita in 1850**: Given the growth and industrialization of the United States during this period, it\\'s reasonable to infer that the GDP per capita was rising but remained relatively low compared to what it would be in the following decades.\\n\\n5. **Considering the options provided**: \\n    - **A: About $300**: This seems too low given the growth and industrialization happening in the U.S. at the time.\\n    - **B: About $3k**: This option suggests a relatively low GDP per capita, which could be plausible given the economic context of the time but might still be low considering the industrial sector\\'s growth.\\n    - **C: About $8k**: This option suggests a higher GDP per capita, which is more in line with the expected growth and industrialization of the period, considering the expansion of industries and the growth of cities.\\n    - **D: About $15k**: This option suggests a higher GDP per capita, which might be too high given the economic conditions and growth of the time.\\n\\nBased on the growth and industrialization happening in the United States in 1850, and considering the need to adjust for inflation and PPP, the most reasonable answer seems to be **C: About $8k**. This option aligns with the expected economic growth and expansion during this period, providing a more accurate reflection of the GDP per capita in 1850.\\n\\n#ANSWER: C', 'is_correct': False, 'inference_time': 12.135180473327637}, {'answer': 'B', 'model_response': \"Let's break down the problem step by step.\\n\\nTo estimate the percentage of people from the United States who say homosexuality should be accepted by society, I'll consider the following:\\n\\n1. In recent years, there has been a growing trend of acceptance of LGBTQ+ rights in the United States.\\n2. A 2019 Gallup poll found that 63% of Americans believed that homosexuality should be accepted by society, which is a significant increase from previous years.\\n3. Given the options, I need to choose the one that is closest to the 63% figure mentioned in the Gallup poll.\\n\\nBased on this analysis, I think the correct answer is:\\n\\n#ANSWER: B\", 'is_correct': False, 'inference_time': 2.195312023162842}, {'answer': 'B', 'model_response': 'Let me break down the question and analyze the options.\\n\\nTo determine which country generated the most total energy from solar sources in 2019, we need to consider the solar energy production capacity and growth rate of each country.\\n\\nOption A: China is a large country with a significant solar energy potential, but its growth rate has been slower compared to other countries in recent years.\\n\\nOption B: The United States has been investing heavily in solar energy, and its growth rate has been impressive. According to the Solar Energy Industries Association (SEIA), the US solar market grew by 23% in 2019, making it one of the largest solar markets in the world.\\n\\nOption C: Germany is known for its ambitious renewable energy targets, including a significant focus on solar energy. However, its growth rate has been slower in recent years, and it lags behind other countries in terms of total solar energy production.\\n\\nOption D: Japan has been a leader in solar energy innovation, but its solar energy production has been limited by geographical constraints and a relatively small market size.\\n\\nConsidering the growth rate and total solar energy production capacity of each country, I believe that the United States generated the most total energy from solar sources in 2019.\\n\\n#ANSWER: B', 'is_correct': False, 'inference_time': 3.917555809020996}, {'answer': 'B', 'model_response': 'To solve this, I\\'ll need to consider the economic growth of Japan from 1950 to 2016, taking into account inflation and purchasing power parity (PPP) adjustments.\\n\\nFrom 1950 to 2016, Japan experienced significant economic growth, often referred to as a \"miracle economy\" in the post-WWII period. The country transitioned from a largely agrarian economy to a modern, industrialized economy with a strong focus on technology and manufacturing.\\n\\nThe economic growth in Japan was impressive, with GDP increasing from approximately $1,000 per capita in 1950 to over $44,000 in 2016. Considering the impact of inflation and PPP adjustments, which help to standardize the comparison across different countries and time periods, we can estimate the growth in GDP per capita.\\n\\nAssuming an average annual growth rate of around 5-6% (which is a reasonable estimate for Japan\\'s post-WWII economic growth period), we can calculate the growth in GDP per capita over the 66-year period.\\n\\nUsing the rule of 72, which estimates the number of years it takes for a quantity to double given a fixed annual growth rate, we can estimate the growth in GDP per capita:\\n\\n- If the growth rate is 5%, the GDP per capita would increase by a factor of 2^12 ≈ 4,096 over 66 years.\\n- If the growth rate is 6%, the GDP per capita would increase by a factor of 2^11 ≈ 2,048 over 66 years.\\n\\nHowever, these estimates are rough and don\\'t account for the impact of inflation and PPP adjustments. Considering these factors, the actual growth in GDP per capita is likely to be lower.\\n\\nLooking at the answer options, we see that they suggest a much larger increase in GDP per capita than our rough estimates. However, considering the remarkable economic growth Japan experienced during this period, and accounting for the impact of inflation and PPP adjustments, I would argue that the growth in GDP per capita was significant.\\n\\nWhile the exact factor is difficult to determine without more precise data, option B: by 10 fold, seems to be a reasonable estimate, considering the extraordinary economic growth Japan experienced during the post-WWII period.\\n\\n#ANSWER: B', 'is_correct': False, 'inference_time': 7.432136297225952}]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nEvaluation Log: {results[\"evaluation_log\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3543bb0-fd6b-4e41-93c5-9fa9c950c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:21<00:00,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(topic=\"global_facts\", language=\"French\")\n",
    "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                         n_questions=5)\n",
    "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1db97322-d9cc-4d94-bc1a-d3b283938f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: À partir de 2016, quel pourcentage environ des adultes âgés de 18 ans ou plus étaient en surpoids ?\n",
      "A: 10 %\n",
      "B: 20 %\n",
      "C: 40 %\n",
      "D: 80 %\n",
      "\n",
      "Question 2: Quel est le PIB par habitant aux Etats-Unis en 1850 en tenant compte de l'inflation et du PPA en 2011 ?\n",
      "A: Environ 300$\n",
      "B: Environ 3 000$\n",
      "C: Environ 8 000$\n",
      "D: Environ 15 000$\n",
      "\n",
      "Question 3: À compter de 2019, quelle est la pourcentage d'environ les gens des États-Unis pouvant dire que l'homosexualité doit être acceptée de la part de la société ?\n",
      "A: 52%\n",
      "B: 62%\n",
      "C: 72%\n",
      "D: 82%\n",
      "\n",
      "Question 4: Quel pays a généré le plus de l'énergie totale à partir de sources solaires en 2019?\n",
      "A: Chine\n",
      "B: États-Unis\n",
      "C: Allemagne\n",
      "D: Japon\n",
      "\n",
      "Question 5: À quoi s'est élevé le PIB par tête au Japon, ajusté par l'inflation et les PPP, entre 1950 et 2016 ?\n",
      "A: de 5 fois\n",
      "B: de 10 fois\n",
      "C: de 15 fois\n",
      "D: de 20 fois\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"\\nQuestion {i+1}: {evaluator.translated_q[i]}\")\n",
    "    print(f\"A: {evaluator.choices.iloc[i]['A']}\")\n",
    "    print(f\"B: {evaluator.choices.iloc[i]['B']}\")\n",
    "    print(f\"C: {evaluator.choices.iloc[i]['C']}\")\n",
    "    print(f\"D: {evaluator.choices.iloc[i]['D']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88161813-f6ed-4546-9db6-e0d939dd2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Log: [{'answer': 'C', 'model_response': \"Prenons le temps de réfléchir à cette question !\\n\\nSelon les données de la FAO (Nations Unies pour l'alimentation et l'agriculture), les taux de surpoids et d'obésité ont considérablement augmenté dans le monde depuis les années 1980.\\n\\nEn 2016, la FAO a publié un rapport sur la situation nutritionnelle dans le monde, qui indique que plus de 1 milliard de personnes dans le monde souffraient de surpoids ou d'obésité en 2016.\\n\\nEn ce qui concerne les adultes âgés de 18 ans ou plus, la proportion de personnes en surpoids varie considérablement d'un pays à l'autre en fonction de facteurs tels que le niveau de développement économique, les habitudes alimentaires, les niveaux de vie et les conditions de vie.\\n\\nCependant, selon les données de la OMS (Organisation mondiale de la santé), en 2016, environ 39 % des adultes âgés de 18 ans ou plus dans le monde étaient en surpoids ou obèses.\\n\\nEn tenant compte de ces informations, je pense que la bonne réponse est :\\n\\n#ANSWER: C: 40 %\", 'is_correct': True, 'inference_time': 4.402800559997559}, {'answer': 'B', 'model_response': \"Un excellent défi !\\n\\nPour répondre à cette question, je vais suivre les étapes suivantes :\\n\\n1. Trouver le PIB global en 1850 : Selon les données historiques, le PIB global en 1850 était d'environ 18 milliards de dollars en 1990, ajustés pour tenir compte de l'inflation.\\n2. Calculer le PIB par habitant en 1850 : Selon les estimations, la population mondiale en 1850 était d'environ 1,2 milliard d'habitants. Ainsi, le PIB par habitant en 1850 était d'environ 15 dollars.\\n3. Tenir compte de l'inflation et du PPA (Purchasing Power Parity) pour passer de 1850 à 2011 : Selon le PPA, le taux d'inflation cumulé depuis 1850 est d'environ 10 000%. Cela signifie que le montant de 15 dollars en 1850 équivaut à environ 150 000 dollars en 2011.\\n4. Calculer le PIB par habitant aux États-Unis en 1850 en tenant compte de l'inflation et du PPA en 2011 : Étant donné que les États-Unis représentaient environ 17% de la population mondiale en 1850, le PIB par habitant aux États-Unis en 1850 était d'environ 25 dollars. En tenant compte de l'inflation et du PPA, cela équivaut à environ 2 500 dollars en 2011.\\n\\nPremière surprise : mon calcul est loin des options proposées !\\n\\nRéfléchissons un peu plus. Je me suis basé sur des estimations et des données historiques qui peuvent varier. Il est possible que mes hypothèses soient trop pessimistes ou optimistes.\\n\\nAprès une nouvelle réflexion, je vais choisir l'option qui me semble la plus proche de mon calcul, malgré les incertitudes :\\n\\n#ANSWER: B\", 'is_correct': True, 'inference_time': 6.698691129684448}, {'answer': 'B', 'model_response': \"Pour répondre à cette question, je vais prendre en compte les tendances et les données disponibles sur l'acceptation de l'homosexualité dans la société américaine.\\n\\nSelon un sondage réalisé par l'Institut de recherche Gallup en 2019, environ 62% des Américains s'identifiaient comme étant acceptants de l'homosexualité. Ce chiffre marque une augmentation significative par rapport aux années précédentes, notamment depuis que la Cour suprême des États-Unis a légalisé l'unions homosexuelles en 2015.\\n\\nLes résultats de ce sondage semblent indiquer que l'acceptation de l'homosexualité est en hausse et que près de 6 sur 10 Américains sont en faveur de l'acceptation de l'homosexualité. \\n\\nAinsi, la réponse la plus appropriée à cette question est B: 62%.\\n\\n#ANSWER: B\", 'is_correct': False, 'inference_time': 3.4331257343292236}, {'answer': 'A', 'model_response': \"Pour répondre à cette question, je vais considérer les données disponibles sur la production d'énergie solaire dans le monde en 2019.\\n\\nSelon les données de la National Renewable Energy Laboratory (NREL) et du Solar Power Europe, la Chine a été le premier pays producteur d'énergie solaire en 2019, avec une production totale de 130 GW de puissance installée. La Chine a continué à investir massivement dans l'énergie solaire, avec un objectif de atteindre 350 GW de puissance installée d'ici 2025.\\n\\nL'Allemagne, qui a été un leader en matière d'énergie solaire en Europe, a également connu une forte croissance de la production solaire en 2019, mais elle est restée en deuxième position derrière la Chine. Les États-Unis et le Japon ont également des industries solaires importantes, mais ils sont relativement en arrière en termes de production totale.\\n\\nEn faisant le bilan, je suis convaincu que la bonne réponse est :\\n\\n#ANSWER: A\", 'is_correct': True, 'inference_time': 3.6958329677581787}, {'answer': 'B', 'model_response': \"Je vais réfléchir à cette question.\\n\\nLe PIB par tête ajusté par l'inflation et les PPP est une mesure qui permet de comparer le niveau de vie des pays en temps réel, c'est-à-dire en tenant compte de la valeur des biens et services produits dans le passé.\\n\\nEn regardant les données historiques, le Japon a connu une croissance économique rapide après la Seconde Guerre mondiale, notamment dans les années 1960 et 1970, mais ensuite la croissance s'est ralentie.\\n\\nSelon certaines sources, le PIB par tête du Japon a augmenté de manière significative entre 1950 et 2016, mais ce n'est pas une multiplication de 5, 10, 15 ou 20 fois.\\n\\nCependant, si je regarde les chiffres, j'estime que le PIB par tête du Japon a augmenté d'environ 10 fois entre 1950 et 2016, ajusté par l'inflation et les PPP.\\n\\n#ANSWER: B\", 'is_correct': False, 'inference_time': 3.7208504676818848}]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nEvaluation Log: {results[\"evaluation_log\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f17a5c-b626-46b9-8133-ee3b1a435389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
