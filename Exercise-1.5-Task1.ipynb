{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ffd2a0-f3aa-49ad-854b-3d48e4bdefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time # Chong Ming. For Exercise task. To measure Inference Latency\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "with open(\"nebius_api_key\", \"r\") as file:\n",
    "    nebius_api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Nebius uses the same OpenAI() class, but with additional details\n",
    "nebius_client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f42a8aa-bea6-4075-aead-96da9c18a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_string(text, max_line_length=80):\n",
    "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
    "\n",
    "    Args:\n",
    "        text: The string to print.\n",
    "        max_line_length: The maximum length of each line.\n",
    "    \"\"\"\n",
    "\n",
    "    output_lines = []\n",
    "    lines = text.split(\"\\n\") #Split the chunk of text retrieved from LLM into lines\n",
    "    for line in lines:       #Loop all the lines\n",
    "        current_line = \"\"\n",
    "        words = line.split() #Split the lines into words separate by whitespace\n",
    "        for word in words:\n",
    "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
    "                current_line += word + \" \"\n",
    "            else:\n",
    "                output_lines.append(current_line.strip())\n",
    "                current_line = word + \" \"\n",
    "        output_lines.append(current_line.strip())  # Append the last line\n",
    "    return \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ee80c7-2d41-483e-8901-ed8038ee97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_llm(prompt: str,\n",
    "                    system_prompt,\n",
    "                    max_tokens=2056,\n",
    "                    client=nebius_client,\n",
    "                    model=\"\",\n",
    "                    prettify=False,\n",
    "                    temperature=0.7) -> str:\n",
    "\n",
    "    messages = []\n",
    "    #print(\"\\nModel Type in answer_with_llm: \"+model+\"\\n\")\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if prettify:\n",
    "        return prettify_string(completion.choices[0].message.content)\n",
    "    else:\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4d1b01-6a07-471f-9b14-1f14a8944ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "This code builds an LLM evaluation tool using the MMLU dataset, a benchmark set of multiple-choice questions get from hugging face. The goal is to:\n",
    "Ask the model questions, Evaluate its answers, Measure how accurate the model is on a specific topic.\n",
    "This class runs the whole evaluation process.\n",
    "'''\n",
    "class MMLUEvaluator:\n",
    "    #Initializes the evaluator with:\n",
    "    #a system prompt (model’s role/instructions), a custom user prompt, a topic.\n",
    "    \n",
    "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
    "                 topic: str = \"high_school_mathematics\"):\n",
    "        \"\"\"\n",
    "        Initialize the MMLU evaluator.\n",
    "\n",
    "        Args:\n",
    "            system_prompt: Optional system prompt for the model\n",
    "            prompt: Custom prompt for the model\n",
    "            topic: Which topic to choose\n",
    "        \"\"\"\n",
    "\n",
    "        self.topic = topic\n",
    "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
    "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
    "\n",
    "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
    "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
    "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
    "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER OPTIONS:\n",
    "A: {A}\n",
    "B: {B}\n",
    "C: {C}\n",
    "D: {D}\n",
    "\"\"\"\n",
    "\n",
    "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
    "\n",
    "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load MMLU test data on a given topic.\n",
    "\n",
    "        Args:\n",
    "            topic: Which topic to choose\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with questions and answers\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
    "\n",
    "        dataset = dataset\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "\n",
    "        # Load questions and choices separately\n",
    "        questions = dataset[\"question\"]\n",
    "        choices = pd.DataFrame(\n",
    "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
    "        )\n",
    "        # In the dataset, true answer labels are in 0-3 format;\n",
    "        # We convert it to A-D\n",
    "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
    "\n",
    "        return questions, choices, answers\n",
    "\n",
    "    def extract_answer(self, solution: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the letter answer from model's response. Example: If the model writes #ANSWER: C, this grabs the C.\n",
    "\n",
    "        Args:\n",
    "            response: Raw model response\n",
    "\n",
    "        Returns:\n",
    "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
    "        \"\"\"\n",
    "        # Look for a single letter answer in the response\n",
    "        try:\n",
    "            #print(\"Print solution: \"+solution)\n",
    "            #answer = solution.split('#ANSWER:')[1].strip()\n",
    "        #except:\n",
    "         #   answer = \"Failed to parse\"\n",
    "        #return answer\n",
    "            \n",
    "            #the above codes has a flaw where if LLM response contains more than 1 \"#ANSWER:\" not matching the real answer, accuracy become zero.\n",
    "            #Modify the codes to look for #ANSWER: followed by optional spaces, then a capital letter A-D.\n",
    "            match = re.search(r\"#ANSWER:\\s*([A-D])\", solution)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return \"Failed to parse\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting answer: {e}\")\n",
    "            return \"Failed to parse\"\n",
    "\n",
    "\n",
    "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
    "                                 correct_answer: str,\n",
    "                                 client, model) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Evaluate a single question. Sends one question to the model.\n",
    "        Collects its answer compares it to the correct answer and return if:\n",
    "        Whether the answer was correct,\n",
    "        The chosen answer,\n",
    "        The model's full response.\n",
    "\n",
    "        Args:\n",
    "            question: Formatted question string\n",
    "            correct_answer: Correct answer letter\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (is_correct, extracted_answer, model_response)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()  # Chong Ming. For Exercise task. Add this line to measure inference latency. Start timer\n",
    "            model_response = answer_with_llm(\n",
    "                prompt=self.prompt.format(\n",
    "                    client=client, model=model,\n",
    "                    topic_prettified=self.topic_prettified,\n",
    "                    question=question,\n",
    "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
    "                ),\n",
    "                system_prompt=self.system_prompt,\n",
    "                model = model, #Put this in to insert in answer_with_llm function\n",
    "                prettify=False\n",
    "            )\n",
    "            # Chong Ming. For Exercise task. Elapsed time\n",
    "            end_time = time.time()  # End timer\n",
    "            inference_time = end_time - start_time\n",
    "            \n",
    "            answer = self.extract_answer(model_response)\n",
    "            is_correct = (answer.upper() == correct_answer.upper())\n",
    "            #return is_correct, answer, model_response\n",
    "\n",
    "            # Chong Ming. For Exercise task. Modify with additional inference_time for return\n",
    "            return is_correct, answer, model_response, inference_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating question: {e}\")\n",
    "            return False, None, None\n",
    "\n",
    "    def run_evaluation(self, client=nebius_client, model=\"none\",\n",
    "                       n_questions=50) -> Dict:\n",
    "        \"\"\"\n",
    "        Runs evaluation on the first n questions (default: 50)\n",
    "        Loops through each question, checks if the model’s answer is right\n",
    "        Calculates the accuracy (correct answers ÷ total)\n",
    "        Returns an accuracy score and all the answers/responses in a log\n",
    "\n",
    "        Args\n",
    "            client: Which client to use (OpenAI or Nebius)\n",
    "            model: Which model to use\n",
    "            n_questions: How many first questions to take\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        evaluation_log = []\n",
    "        correct_count = 0\n",
    "        inference_times = [] #Chong Ming. For Exercise task. to calculate average inference time, store inference_time in a list. Method 1\n",
    "        \n",
    "        total_inference_time = 0.0   #Chong Ming. For Exercise task. For purpose of collecting the time and derived an average. Method 2\n",
    "        \n",
    "        #print(\"The model in run_evaluation: \"+model)\n",
    "        #print(\"The model in n_questions: \"+str(n_questions))\n",
    "\n",
    "        if n_questions:\n",
    "            n_questions = min(n_questions, len(self.questions))\n",
    "        else:\n",
    "            n_questions = len(self.questions)\n",
    "\n",
    "        for i in tqdm(range(n_questions)):\n",
    "            is_correct, answer, model_response, inference_time = self.evaluate_single_question(\n",
    "                question=self.questions[i],\n",
    "                choices=self.choices.iloc[i],\n",
    "                correct_answer=self.answers[i],\n",
    "                client=client,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "                \n",
    "            evaluation_log.append({\n",
    "                'answer': answer,\n",
    "                'model_response': model_response,\n",
    "                'is_correct': is_correct,\n",
    "                'inference_time': inference_time #Chong Ming. For Exercise. Append only interence time for Method 1.\n",
    "            })\n",
    "        \n",
    "        #Chong Ming. For Exercise task. Adding up the time. Method 1\n",
    "        total_inference_time += inference_time \n",
    "        \n",
    "        #Chong Ming. For Exercise task. To keep adding into 'inference_times' list. Method 2    \n",
    "        inference_times.append(inference_time) \n",
    "\n",
    "        accuracy = correct_count / n_questions\n",
    "        avg_inference_time = total_inference_time / n_questions #Chong Ming. Calculate average. Method 1\n",
    "        avg_inference_time2 = sum(inference_times) / len(inference_times) if inference_times else 0. # Method 2\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'accuracy': accuracy,\n",
    "            'evaluation_log': evaluation_log,\n",
    "            'avg_inference_time': avg_inference_time, #Chong Ming. Add in to return additional info on the time results\n",
    "            'avg_inference_time_Method2': avg_inference_time2, #Chong Ming. Add in to return additional info on the time results in list\n",
    "        }\n",
    "\n",
    "        return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1faa77f-3003-4669-8a0d-e612f749f6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:44<00:00, 44.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(topic=\"global_facts\")\n",
    "results = evaluator.run_evaluation(model=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "                         n_questions=1)\n",
    "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a006d6f-dec7-419f-93c6-013b895b221b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Log: [{'answer': 'C', 'model_response': '<think>\\nFirst, the question is: \"As of 2016, about what percentage of adults aged 18 years or older were overweight?\" The options are A: 10%, B: 20%, C: 40%, D: 80%.\\n\\nI need to recall global facts about overweight statistics. I remember that the World Health Organization (WHO) and other health organizations provide data on this. The keyword is \"as of 2016,\" so I should think about data from around that year.\\n\\nFrom my knowledge, overweight and obesity are major global health issues. According to WHO, in 2016, more than 1.9 billion adults were overweight, and of these, over 650 million were obese. But I need the percentage of adults who are overweight, which includes both overweight and obese individuals? No, overweight is a category that often includes pre-obesity, but let\\'s clarify the definitions.\\n\\nTypically, overweight is defined as having a Body Mass Index (BMI) of 25 or higher, and obesity is a subset with BMI 30 or higher. So the percentage for overweight should include those with BMI >=25.\\n\\nNow, what was the global percentage in 2016? I think I\\'ve seen that around 39% or 40% of adults were overweight. Let me think.\\n\\nI recall that in 2016, WHO reported that approximately 39% of adults aged 18 years and over were overweight. That sounds familiar. Let me double-check.\\n\\nI should also consider that this might be an average global figure. It varies by region, but the question is about global percentage.\\n\\nOption C is 40%, which seems plausible. But let\\'s look at the options: A:10% – that seems too low. B:20% – also low. D:80% – that seems too high for global average; that might be for specific countries like the US, but not globally.\\n\\nFor example, in the US, around 70% are overweight or obese, but globally it\\'s lower because many countries have lower rates.\\n\\nAccording to WHO data from 2016: \"Worldwide obesity has nearly tripled since 1975. In 2016, more than 1.9 billion adults, 18 years and older, were overweight. Of these over 650 million were obese. Overall, about 13% of the world\\'s adult population (11% of men and 15% of women) were obese in 2016. The prevalence of overweight among adults was 39% in 2016.\"\\n\\nYes, that\\'s what I thought. So 39% is approximately 40%.\\n\\nLet me confirm the source. This is from WHO factsheets. I can cite it mentally.\\n\\nThe question says \"overweight,\" and WHO uses \"overweight\" to refer to BMI >=25, which includes obesity, but in their reports, they often state the overweight percentage as inclusive.\\n\\nIn the quote above, it says \"the prevalence of overweight among adults was 39% in 2016.\" And that includes those who are obese as well, since obesity is a subset.\\n\\nSometimes, people separate overweight (BMI 25-29.9) and obesity (BMI >=30), but the question says \"overweight,\" which might be ambiguous. However, in global health contexts, \"overweight\" often refers to BMI >=25.\\n\\nLooking back at the WHO data: they say \"1.9 billion adults were overweight,\" and that\\'s BMI >=25, including obese.\\n\\nThen, 1.9 billion out of how many adults? Global adult population in 2016.\\n\\nWorld population in 2016 was about 7.4 billion. Adults aged 18 and over might be around 5 billion or so? Let\\'s estimate.\\n\\nRoughly, about 70% of the world population is adults (ages 15-64, but 18 and over might be similar). In 2016, population was about 7.4 billion. The proportion aged 18 and over: I think it\\'s around 65-70%. For simplicity, WHO gives the percentage directly as 39%, so I don\\'t need to calculate.\\n\\nWHO states: \"In 2016, 39% of adults aged 18 years and over (39% of men and 40% of women) were overweight.\"\\n\\nYes, so it\\'s 39%, which rounds to about 40%.\\n\\nTherefore, option C: 40% is correct.\\n\\nBut let\\'s see why the other options are wrong.\\n\\nA: 10% – that\\'s way too low. Even in regions with low overweight rates, like some parts of Asia or Africa, the global average is much higher.\\n\\nB: 20% – still low. For example, in 1975, it was around 20%, but by 2016, it had increased significantly.\\n\\nD: 80% – that\\'s only in high-income countries or specific places. Globally, it\\'s not that high.\\n\\nFor instance, in the US, about 70-75% are overweight or obese, but globally, it\\'s lower.\\n\\nSo, based on WHO data, 39% is the figure.\\n\\nI recall that the NCD Risk Factor Collaboration also has data, but it aligns with WHO.\\n\\nTherefore, I think C: 40% is the answer.\\n\\nThe question says \"as of 2016,\" and WHO uses that data.\\n\\nAlso, the answer should be justified.\\n\\nSo, for the response, I need to ponder and justify, then give the answer.\\n</think>\\nThe question asks for the percentage of adults aged 18 years or older who were overweight globally as of 2016. To answer this, I need to rely on authoritative global health data, particularly from sources like the World Health Organization (WHO), which tracks such statistics.\\n\\n- Overweight is typically defined as having a Body Mass Index (BMI) of 25 or higher, which includes the obese category (BMI ≥ 30) in most global health reports.\\n- According to the WHO, in 2016, approximately 1.9 billion adults worldwide were overweight, including those who were obese. This represented about 39% of the global adult population aged 18 and older. Specifically, the WHO\\'s 2016 data shows that 39% of men and 40% of women fell into this category.\\n- Reviewing the options:\\n  - A: 10% is too low, as even regions with the lowest rates (e.g., parts of Southeast Asia or Africa) had higher averages, and globally, rates have risen significantly since the 1970s.\\n  - B: 20% is also low; this was closer to the global average in the 1980s, but by 2016, rates had nearly doubled due to factors like urbanization, diet changes, and reduced physical activity.\\n  - C: 40% aligns closely with the WHO\\'s 39% figure, which is often rounded to 40% in summaries and reports for simplicity.\\n  - D: 80% is unrealistically high for the global average; this may reflect rates in specific high-income countries (e.g., the United States, where about 70-75% of adults were overweight or obese in 2016), but it does not represent the worldwide figure, which includes lower rates in many developing regions.\\n\\nThe WHO is a primary source for global health metrics, and their data is widely cited in similar contexts. Other sources, such as the Global Burden of Disease study, corroborate this approximate percentage. Therefore, based on the evidence, option C is the most accurate.\\n\\n#ANSWER: C', 'is_correct': True, 'inference_time': 44.41864991188049}]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nEvaluation Log: {results[\"evaluation_log\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae27884-980e-42d5-8e27-b12d02df8fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg_Inference_Time: 44.42secs\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAvg_Inference_Time: {results[\"avg_inference_time\"]:.2f}secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f2f39f-7a8f-4825-b327-3592b7dedde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg_Inference_Time Method 2: 44.42secs\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAvg_Inference_Time Method 2: {results[\"avg_inference_time_Method2\"]:.2f}secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4245c-2279-4530-8f00-9e0b58db9852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
