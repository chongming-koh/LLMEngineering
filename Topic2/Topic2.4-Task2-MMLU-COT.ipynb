{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d795c92-6a83-4787-9bef-a0dec358a020",
   "metadata": {},
   "source": [
    "# Load Nebius API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8dd5cc-61be-4b7d-b895-04751ac0195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- SETUP ---\n",
    "\n",
    "# Load Nebius API key\n",
    "with open(\"nebius_api_key\", \"r\") as file:\n",
    "    nebius_api_key = file.read().strip()\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
    "\n",
    "# Create Nebius client\n",
    "nebius_client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6369c11-2b20-452c-a39a-267025fb6073",
   "metadata": {},
   "source": [
    "# Prompt variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e83619-4e7a-4872-b9f6-8ebba594baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cot_suppression = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
    "Pick the best answer, and write only the answer letter after #ANSWER:.\n",
    "QUESTION: {question}\n",
    "ANSWER OPTIONS:\n",
    "A: {A}\n",
    "B: {B}\n",
    "C: {C}\n",
    "D: {D}\n",
    "#ANSWER:\"\"\"\n",
    "\n",
    "prompt_basic_cot = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
    "Think step by step and explain your reasoning. Then, write the chosen answer letter A, B, C, or D after #ANSWER:.\n",
    "QUESTION: {question}\n",
    "ANSWER OPTIONS:\n",
    "A: {A}\n",
    "B: {B}\n",
    "C: {C}\n",
    "D: {D}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf92b95-b07c-48db-8a64-813b2c324775",
   "metadata": {},
   "source": [
    "# Cost settings per 1M tokens (Nebius, sample values, adjust if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e41973-c436-400b-9399-059e842bb9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = {\n",
    "    '70B': {'input': 0.4, 'output': 1.2},\n",
    "    '8B': {'input': 0.02, 'output': 0.06}\n",
    "}\n",
    "def compute_cost(model_name, input_tokens, output_tokens):\n",
    "    key = '70B' if '70B' in model_name else '8B'\n",
    "    input_cost = input_tokens / 1_000_000 * costs[key]['input']\n",
    "    output_cost = output_tokens / 1_000_000 * costs[key]['output']\n",
    "    return input_cost + output_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4be416-8c94-4a54-96f8-4546fc8de589",
   "metadata": {},
   "source": [
    "# --- MMLU Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c59fc46-d99e-4118-8919-1def1b8f7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMLUEvaluator:\n",
    "    def __init__(self, system_prompt=None, prompt=None, topic=\"high_school_mathematics\"):\n",
    "        self.topic = topic\n",
    "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
    "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
    "        self.prompt = prompt or \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
    "                                    You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
    "                                    At the end, do write the chosen answer option A, B, C, D after #ANSWER:\"\"\"\n",
    "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
    "\n",
    "    def load_mmlu_data(self, topic: str):\n",
    "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "        questions = dataset[\"question\"]\n",
    "        choices = pd.DataFrame(dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"])\n",
    "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
    "        return questions, choices, answers\n",
    "\n",
    "    def extract_answer(self, solution: str) -> str:\n",
    "        try:\n",
    "            match = re.search(r\"#ANSWER:\\s*([A-D])\", solution)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return \"Failed to parse\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting answer: {e}\")\n",
    "            return \"Failed to parse\"\n",
    "\n",
    "    def evaluate_single_question(self, question, choices, correct_answer, client, model):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            formatted_prompt = self.prompt.format(\n",
    "                topic_prettified=self.topic_prettified,\n",
    "                question=question,\n",
    "                A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
    "            )\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "                ],\n",
    "                max_tokens=2056,\n",
    "                temperature=0.7  # Default, can override\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            model_response = completion.choices[0].message.content\n",
    "            answer = self.extract_answer(model_response)\n",
    "            is_correct = (answer.upper() == correct_answer.upper())\n",
    "            usage = completion.usage\n",
    "            inference_time = end_time - start_time\n",
    "            return is_correct, answer, model_response, inference_time, usage\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating question: {e}\")\n",
    "            return False, None, None, 0, None\n",
    "\n",
    "    def run_evaluation(self, client, model, n_questions=50):\n",
    "        evaluation_log = []\n",
    "        correct_count = 0\n",
    "        total_inference_time = 0.0\n",
    "        total_input_tokens = 0\n",
    "        total_output_tokens = 0\n",
    "\n",
    "        for i in tqdm(range(n_questions)):\n",
    "            is_correct, answer, model_response, inference_time, usage = self.evaluate_single_question(\n",
    "                question=self.questions[i],\n",
    "                choices=self.choices.iloc[i],\n",
    "                correct_answer=self.answers[i],\n",
    "                client=client,\n",
    "                model=model\n",
    "            )\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            if usage:\n",
    "                total_input_tokens += usage.prompt_tokens\n",
    "                total_output_tokens += usage.completion_tokens\n",
    "            total_inference_time += inference_time\n",
    "            evaluation_log.append({\n",
    "                'question': self.questions[i],\n",
    "                'answer': answer,\n",
    "                'model_response': model_response,\n",
    "                'is_correct': is_correct,\n",
    "                'inference_time': inference_time,\n",
    "                'input_tokens': usage.prompt_tokens if usage else 0,\n",
    "                'output_tokens': usage.completion_tokens if usage else 0,\n",
    "                'correct_answer': self.answers[i]\n",
    "            })\n",
    "        accuracy = correct_count / n_questions\n",
    "        avg_inference_time = total_inference_time / n_questions\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'evaluation_log': evaluation_log,\n",
    "            'avg_inference_time': avg_inference_time,\n",
    "            'total_input_tokens': total_input_tokens,\n",
    "            'total_output_tokens': total_output_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2618a3-f3e9-4de5-9c48-2d0115647657",
   "metadata": {},
   "source": [
    "# --------- EXPERIMENT SETUPS ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56296dbf-5bde-4676-bd7e-eb54b00ce5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"high_school_mathematics\"  # or your chosen math topic\n",
    "n_questions = 50\n",
    "\n",
    "models = {\n",
    "    '8B': \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    '70B': \"meta-llama/Meta-Llama-3.1-70B-Instruct\" \n",
    "}\n",
    "results_summary = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a36c52-d3b9-421b-b453-893284eafeb3",
   "metadata": {},
   "source": [
    "# --- 1. Llama-3.1-70B, CoT Suppression ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7d9016-943d-42f8-836d-c139607e8bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:04<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(system_prompt=None, prompt=prompt_cot_suppression, topic=topic)\n",
    "results_70b_cots = evaluator.run_evaluation(\n",
    "    client=nebius_client, model=models['70B'], n_questions=n_questions)\n",
    "results_summary.append({\n",
    "    \"model\": \"Llama-3.1-70B\", \"strategy\": \"CoT Suppression\",\n",
    "    \"accuracy\": results_70b_cots[\"accuracy\"],\n",
    "    \"cost\": compute_cost(models['70B'], results_70b_cots[\"total_input_tokens\"], results_70b_cots[\"total_output_tokens\"])\n",
    "})\n",
    "with open('results_70b_cotSuppress.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_70b_cots, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99eb0f5-b7b5-429c-8fe2-396fc35fac5a",
   "metadata": {},
   "source": [
    "# --- 2. Llama-3.1-8B, CoT Suppression ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8930831-0ca5-45cc-9a50-09aedb44edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:15<00:00,  3.14it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(system_prompt=None, prompt=prompt_cot_suppression, topic=topic)\n",
    "results_8b_cots = evaluator.run_evaluation(\n",
    "    client=nebius_client, model=models['8B'], n_questions=n_questions)\n",
    "results_summary.append({\n",
    "    \"model\": \"Llama-3.1-8B\", \"strategy\": \"CoT Suppression\",\n",
    "    \"accuracy\": results_8b_cots[\"accuracy\"],\n",
    "    \"cost\": compute_cost(models['8B'], results_8b_cots[\"total_input_tokens\"], results_8b_cots[\"total_output_tokens\"])\n",
    "})\n",
    "with open('results_8b_cotSuppress.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_8b_cots, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1c3b-a756-4912-95e9-3f6813057473",
   "metadata": {},
   "source": [
    "# --- 3. Llama-3.1-70B, Basic CoT ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1e64b9-55a3-46ac-acd6-09b8b4728f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:18<00:00, 14.77s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(system_prompt=None, prompt=prompt_basic_cot, topic=topic)\n",
    "results_70b_cot = evaluator.run_evaluation(\n",
    "    client=nebius_client, model=models['70B'], n_questions=n_questions)\n",
    "results_summary.append({\n",
    "    \"model\": \"Llama-3.1-70B\", \"strategy\": \"Basic CoT\",\n",
    "    \"accuracy\": results_70b_cot[\"accuracy\"],\n",
    "    \"cost\": compute_cost(models['70B'], results_70b_cot[\"total_input_tokens\"], results_70b_cot[\"total_output_tokens\"])\n",
    "})\n",
    "with open('results_70b_cot.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_70b_cot, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b1002-d9dc-44cd-8b51-e100007c7bb2",
   "metadata": {},
   "source": [
    "# --- 4. Llama-3.1-8B, Basic CoT ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad70448-eaf5-4339-910a-8419d43f4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:40<00:00, 10.40s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(system_prompt=None, prompt=prompt_basic_cot, topic=topic)\n",
    "results_8b_cot = evaluator.run_evaluation(\n",
    "    client=nebius_client, model=models['8B'], n_questions=n_questions)\n",
    "results_summary.append({\n",
    "    \"model\": \"Llama-3.1-8B\", \"strategy\": \"Basic CoT\",\n",
    "    \"accuracy\": results_8b_cot[\"accuracy\"],\n",
    "    \"cost\": compute_cost(models['8B'], results_8b_cot[\"total_input_tokens\"], results_8b_cot[\"total_output_tokens\"])\n",
    "})\n",
    "with open('results_8b_cot.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_8b_cot, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73228a2-d4b2-41e5-a4d7-d72602a280f1",
   "metadata": {},
   "source": [
    "# --- 5. Llama-3.1-8B, Self-Consistency (majority vote, 5 runs per Q) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf9f066-23fd-4543-bd11-eb103e176c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [35:23<00:00, 42.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Llama-3.1-8B, Self-Consistency (majority vote, 5 runs per Q) ---\n",
    "evaluator = MMLUEvaluator(system_prompt=None, prompt=prompt_basic_cot, topic=topic)\n",
    "all_results_sc = []\n",
    "correct_count = 0\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_inference_time = 0.0\n",
    "for i in tqdm(range(n_questions)):\n",
    "    answers = []\n",
    "    usages = []\n",
    "    model_responses = []\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        is_correct, answer, model_response, inference_time, usage = evaluator.evaluate_single_question(\n",
    "            evaluator.questions[i], evaluator.choices.iloc[i], evaluator.answers[i],\n",
    "            nebius_client, models['8B'])\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        times.append(inference_time)\n",
    "        if usage:\n",
    "            usages.append(usage)\n",
    "    # Majority vote (excluding Failed to parse)\n",
    "    valid_answers = [a for a in answers if a and a in \"ABCD\"]\n",
    "    majority = Counter(valid_answers).most_common(1)[0][0] if valid_answers else \"Failed to parse\"\n",
    "    is_correct = (majority == evaluator.answers[i])\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    total_input_tokens += sum(u.prompt_tokens for u in usages)\n",
    "    total_output_tokens += sum(u.completion_tokens for u in usages)\n",
    "    total_inference_time += np.mean(times)\n",
    "    all_results_sc.append({\n",
    "        'question': evaluator.questions[i],\n",
    "        'answers': answers,\n",
    "        'model_responses': model_responses,\n",
    "        'chosen_answer': majority,\n",
    "        'is_correct': is_correct,\n",
    "        'inference_time': np.mean(times),\n",
    "        'input_tokens': sum(u.prompt_tokens for u in usages),\n",
    "        'output_tokens': sum(u.completion_tokens for u in usages),\n",
    "        'correct_answer': evaluator.answers[i]\n",
    "    })\n",
    "accuracy_sc = correct_count / n_questions\n",
    "avg_inference_time_sc = total_inference_time / n_questions\n",
    "results_summary.append({\n",
    "    \"model\": \"Llama-3.1-8B\", \"strategy\": \"Self-Consistency (5x, Basic CoT)\",\n",
    "    \"accuracy\": accuracy_sc,\n",
    "    \"cost\": compute_cost(models['8B'], total_input_tokens, total_output_tokens)\n",
    "})\n",
    "with open('results_8b_selfconsistency.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results_sc, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11b53a-dcbf-4a6d-879c-51379d7d57b0",
   "metadata": {},
   "source": [
    "# --- SUMMARY TABLE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41090fe-a286-4577-9060-87e61fb8a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Results Summary =====\n",
      "\n",
      "           model                          strategy  accuracy      cost\n",
      "0  Llama-3.1-70B                   CoT Suppression      0.54  0.004493\n",
      "1   Llama-3.1-8B                   CoT Suppression      0.02  0.000149\n",
      "2  Llama-3.1-70B                         Basic CoT      0.82  0.030715\n",
      "3   Llama-3.1-8B                         Basic CoT      0.62  0.002193\n",
      "4   Llama-3.1-8B  Self-Consistency (5x, Basic CoT)      0.76  0.008778\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_summary = pd.DataFrame(results_summary)\n",
    "print(\"\\n===== Results Summary =====\\n\")\n",
    "print(df_summary[[\"model\", \"strategy\", \"accuracy\", \"cost\"]])\n",
    "df_summary.to_csv(\"experiment_results_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a91e1e-821d-4329-941a-517dde688fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
