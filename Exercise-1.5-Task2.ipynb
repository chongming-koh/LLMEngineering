{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ffd2a0-f3aa-49ad-854b-3d48e4bdefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time # Chong Ming. For Exercise task. To measure Inference Latency\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "with open(\"nebius_api_key\", \"r\") as file:\n",
    "    nebius_api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Nebius uses the same OpenAI() class, but with additional details\n",
    "nebius_client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f42a8aa-bea6-4075-aead-96da9c18a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_string(text, max_line_length=80):\n",
    "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
    "\n",
    "    Args:\n",
    "        text: The string to print.\n",
    "        max_line_length: The maximum length of each line.\n",
    "    \"\"\"\n",
    "\n",
    "    output_lines = []\n",
    "    lines = text.split(\"\\n\") #Split the chunk of text retrieved from LLM into lines\n",
    "    for line in lines:       #Loop all the lines\n",
    "        current_line = \"\"\n",
    "        words = line.split() #Split the lines into words separate by whitespace\n",
    "        for word in words:\n",
    "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
    "                current_line += word + \" \"\n",
    "            else:\n",
    "                output_lines.append(current_line.strip())\n",
    "                current_line = word + \" \"\n",
    "        output_lines.append(current_line.strip())  # Append the last line\n",
    "    return \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ee80c7-2d41-483e-8901-ed8038ee97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_llm(prompt: str,\n",
    "                    system_prompt,\n",
    "                    max_tokens=2056,\n",
    "                    client=nebius_client,\n",
    "                    model=\"\",\n",
    "                    prettify=False,\n",
    "                    temperature=0.7) -> str:\n",
    "\n",
    "    messages = []\n",
    "    #print(\"\\nModel Type in answer_with_llm: \"+model+\"\\n\")\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    if prettify:\n",
    "        return prettify_string(completion.choices[0].message.content)\n",
    "    else:\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4d1b01-6a07-471f-9b14-1f14a8944ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "This code builds an LLM evaluation tool using the MMLU dataset, a benchmark set of multiple-choice questions get from hugging face. The goal is to:\n",
    "Ask the model questions, Evaluate its answers, Measure how accurate the model is on a specific topic.\n",
    "This class runs the whole evaluation process.\n",
    "'''\n",
    "class MMLUEvaluator:\n",
    "    #Initializes the evaluator with:\n",
    "    #a system prompt (model’s role/instructions), a custom user prompt, a topic.\n",
    "    \n",
    "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
    "                 topic: str = \"high_school_mathematics\"):\n",
    "        \"\"\"\n",
    "        Initialize the MMLU evaluator.\n",
    "\n",
    "        Args:\n",
    "            system_prompt: Optional system prompt for the model\n",
    "            prompt: Custom prompt for the model\n",
    "            topic: Which topic to choose\n",
    "        \"\"\"\n",
    "\n",
    "        self.topic = topic\n",
    "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
    "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
    "\n",
    "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
    "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
    "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
    "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER OPTIONS:\n",
    "A: {A}\n",
    "B: {B}\n",
    "C: {C}\n",
    "D: {D}\n",
    "\"\"\"\n",
    "\n",
    "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
    "\n",
    "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load MMLU test data on a given topic.\n",
    "\n",
    "        Args:\n",
    "            topic: Which topic to choose\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with questions and answers\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
    "\n",
    "        dataset = dataset\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "\n",
    "        # Load questions and choices separately\n",
    "        questions = dataset[\"question\"]\n",
    "        choices = pd.DataFrame(\n",
    "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
    "        )\n",
    "        # In the dataset, true answer labels are in 0-3 format;\n",
    "        # We convert it to A-D\n",
    "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
    "\n",
    "        return questions, choices, answers\n",
    "\n",
    "    def extract_answer(self, solution: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the letter answer from model's response. Example: If the model writes #ANSWER: C, this grabs the C.\n",
    "\n",
    "        Args:\n",
    "            response: Raw model response\n",
    "\n",
    "        Returns:\n",
    "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
    "        \"\"\"\n",
    "        # Look for a single letter answer in the response\n",
    "        try:\n",
    "            #print(\"Print solution: \"+solution)\n",
    "            #answer = solution.split('#ANSWER:')[1].strip()\n",
    "        #except:\n",
    "         #   answer = \"Failed to parse\"\n",
    "        #return answer\n",
    "            \n",
    "            #the above codes has a flaw where if LLM response contains more than 1 \"#ANSWER:\" not matching the real answer, accuracy become zero.\n",
    "            #Modify the codes to look for #ANSWER: followed by optional spaces, then a capital letter A-D.\n",
    "            match = re.search(r\"#ANSWER:\\s*([A-D])\", solution)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return \"Failed to parse\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting answer: {e}\")\n",
    "            return \"Failed to parse\"\n",
    "\n",
    "\n",
    "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
    "                                 correct_answer: str,\n",
    "                                 client, \n",
    "                                 model,\n",
    "                                 #Chong Ming: add in to accept language, translator_client, and translator_model as arguments\n",
    "                                 language=None, translator_client=None, translator_model=None) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Evaluate a single question. Sends one question to the model.\n",
    "        Collects its answer compares it to the correct answer and return if:\n",
    "        Whether the answer was correct,\n",
    "        The chosen answer,\n",
    "        The model's full response.\n",
    "\n",
    "        Args:\n",
    "            question: Formatted question string\n",
    "            correct_answer: Correct answer letter\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (is_correct, extracted_answer, model_response)\n",
    "        \"\"\"\n",
    "        # Chong Ming: Exercise Task. Calling the Translation function\n",
    "        if language and translator_client:\n",
    "            print(\"Original question:\", question)\n",
    "            question = translate_text(question, language, translator_client, translator_model)\n",
    "            #print(\"Translated questionret retrieved:\", question)\n",
    "            #Chong Ming: To extract the translation question\n",
    "            question = extract_translation_tag(question)\n",
    "            print(\"Translated question:\", question)\n",
    "            \n",
    "            for option in ['A', 'B', 'C', 'D']:\n",
    "                original_text = choices[option]\n",
    "\n",
    "                #Chong Ming: To extract the translated choices\n",
    "                choices[option] = translate_text(choices[option], language, translator_client, translator_model)\n",
    "\n",
    "                translated_text = extract_translation_tag(choices[option]) \n",
    "                choices[option] = extract_translation_tag(choices[option]) \n",
    "                print(f\"Original option {option}: {original_text}\")\n",
    "                print(f\"Translated option {option}: {translated_text}\")\n",
    "\n",
    "                \n",
    "        try:\n",
    "            start_time = time.time()  # Chong Ming. For Exercise task. Add this line to measure inference latency. Start timer\n",
    "            model_response = answer_with_llm(\n",
    "                prompt=self.prompt.format(\n",
    "                    client=client, model=model,\n",
    "                    topic_prettified=self.topic_prettified,\n",
    "                    question=question,\n",
    "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
    "                ),\n",
    "                system_prompt=self.system_prompt,\n",
    "                model = model, #Put this in to insert in answer_with_llm function\n",
    "                prettify=False\n",
    "            )\n",
    "            # Chong Ming. For Exercise task. Elapsed time\n",
    "            end_time = time.time()  # End timer\n",
    "            inference_time = end_time - start_time\n",
    "            \n",
    "            answer = self.extract_answer(model_response)\n",
    "            is_correct = (answer.upper() == correct_answer.upper())\n",
    "            #return is_correct, answer, model_response\n",
    "\n",
    "            # Chong Ming. For Exercise task. Modify with additional inference_time for return\n",
    "            return is_correct, answer, model_response, inference_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating question: {e}\")\n",
    "            return False, None, None\n",
    "\n",
    "    def run_evaluation(self, \n",
    "                       client=nebius_client, \n",
    "                       model=\"none\",\n",
    "                       n_questions=50,\n",
    "                       #Chong Ming for exercise. Add in to accept language, translator_client, and translator_model as arguments:\n",
    "                       language=None,\n",
    "                       translator_client=None,\n",
    "                       translator_model=\"deepseek-ai/DeepSeek-R1\") -> Dict:\n",
    "        \n",
    "        \"\"\"\n",
    "        Runs evaluation on the first n questions (default: 50)\n",
    "        Loops through each question, checks if the model’s answer is right\n",
    "        Calculates the accuracy (correct answers ÷ total)\n",
    "        Returns an accuracy score and all the answers/responses in a log\n",
    "\n",
    "        Args\n",
    "            client: Which client to use (OpenAI or Nebius)\n",
    "            model: Which model to use\n",
    "            n_questions: How many first questions to take\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        evaluation_log = []\n",
    "        correct_count = 0\n",
    "        inference_times = [] #Chong Ming. For Exercise task. to calculate average inference time, store inference_time in a list. Method 1\n",
    "        \n",
    "        total_inference_time = 0.0   #Chong Ming. For Exercise task. For purpose of collecting the time and derived an average. Method 2\n",
    "        \n",
    "        #print(\"The model in run_evaluation: \"+model)\n",
    "        #print(\"The model in n_questions: \"+str(n_questions))\n",
    "\n",
    "        if n_questions:\n",
    "            n_questions = min(n_questions, len(self.questions))\n",
    "        else:\n",
    "            n_questions = len(self.questions)\n",
    "\n",
    "        for i in tqdm(range(n_questions)):\n",
    "            is_correct, answer, model_response, inference_time = self.evaluate_single_question(\n",
    "                question=self.questions[i],\n",
    "                choices=self.choices.iloc[i],\n",
    "                correct_answer=self.answers[i],\n",
    "                client=client,\n",
    "                model=model,\n",
    "                #Chong Ming for exercise. Add in to accept language, translator_client, and translator_model as arguments:\n",
    "                language=language,\n",
    "                translator_client=translator_client,\n",
    "                translator_model=translator_model,\n",
    "            )\n",
    "\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "                \n",
    "            evaluation_log.append({\n",
    "                'answer': answer,\n",
    "                'model_response': model_response,\n",
    "                'is_correct': is_correct,\n",
    "                'inference_time': inference_time #Chong Ming. For Exercise. Append only interence time for Method 1.\n",
    "            })\n",
    "        \n",
    "        #Chong Ming. For Exercise task. Adding up the time. Method 1\n",
    "        total_inference_time += inference_time \n",
    "        print(\"\\ntotal_inference_time: \"+str(total_inference_time))\n",
    "        \n",
    "        #Chong Ming. For Exercise task. To keep adding into 'inference_times' list. Method 2    \n",
    "        inference_times.append(inference_time) \n",
    "\n",
    "        accuracy = correct_count / n_questions\n",
    "        avg_inference_time = total_inference_time / n_questions #Chong Ming. Calculate average. Method 1\n",
    "        avg_inference_time2 = sum(inference_times) / len(inference_times) if inference_times else 0. # Method 2\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'accuracy': accuracy,\n",
    "            'evaluation_log': evaluation_log,\n",
    "            'avg_inference_time': avg_inference_time, #Chong Ming. Add in to return additional info on the time results\n",
    "            'avg_inference_time_Method2': avg_inference_time2, #Chong Ming. Add in to return additional info on the time results in list\n",
    "        }\n",
    "\n",
    "        return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3d471e-2fe8-4db1-a173-ad6490b6d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, target_language, translator_client, translator_model=\"none\"):\n",
    "    prompt = (\n",
    "        f\"Translate the following text into {target_language}. \"\n",
    "        f\"Do not add comments, explanations, or alter the structure. Only the translation:\\n\\n{text}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise translation engine.\"\n",
    "        \"Respond with only the translated text between <translation> and </translation> tags, with nothing else before or after.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    completion = translator_client.chat.completions.create(\n",
    "        model=translator_model,\n",
    "        messages=messages,\n",
    "        max_tokens=2056,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b9c6b8-ca9a-47e3-99b1-2bec74dabfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_translation_tag(text):\n",
    "    \"\"\"Extracts the text within <translation></translation> tags. Returns original if not found.\"\"\"\n",
    "    match = re.search(r\"<translation>(.*?)</translation>\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b21ef39-63eb-40eb-8683-aa108f4143fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: As of 2016, about what percentage of adults aged 18 years or older were overweight?\n",
      "Translated question: tags with pure output. The French equivalent must maintain the interrogative form while adjusting: \"as of\" becomes \"en\", \"about\" translates to \"environ\", and \"overweight\" is accurately rendered as \"en surpoids\". The age phrasing shifts to natural French syntax with \"âgés de 18 ans ou plus\".\n",
      "\n",
      "Double-checking statistical terminology: \"percentage\" becomes \"pourcentage\" without the % symbol since the original lacks it. Ensuring the question mark remains at the end. Final output must be clinically clean - no introductory phrases or even line breaks outside the tags.\n",
      "</think>\n",
      "<translation>En 2016, environ quel pourcentage des adultes âgés de 18 ans ou plus étaient en surpoids ?\n",
      "Original option A: 10%\n",
      "Translated option A: 10 %\n",
      "Original option B: 20%\n",
      "Translated option B: tags. This is consistent with my role as a precise translation engine. \n",
      "\n",
      "I recall that percentage expressions in French use spaces around the percent sign, unlike English. So \"20%\" becomes \"20 %\" in French. \n",
      "\n",
      "The user seems to value minimalism and precision, likely for some formal document or interface where extra text would be problematic. \n",
      "\n",
      "Better double-check the spacing convention - yes, French typography requires a non-breaking space before the percent sign. \n",
      "\n",
      "Simple enough. Just need to output the exact French equivalent in the specified format.\n",
      "</think>\n",
      "<translation>20 %\n",
      "Original option C: 40%\n",
      "Translated option C: 40 %\n",
      "Original option D: 80%\n",
      "Translated option D: 80 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:27<00:00, 87.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total_inference_time: 58.19391846656799\n",
      "\n",
      "Accuracy: 1.0\n",
      "Avg inference time: 58.19391846656799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(topic=\"global_facts\")\n",
    "results = evaluator.run_evaluation(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "    n_questions=1,\n",
    "    language=\"French\",  # or any language\n",
    "    translator_client=nebius_client,\n",
    "    translator_model=\"deepseek-ai/DeepSeek-R1-0528\"\n",
    ")\n",
    "print(f'\\nAccuracy: {results[\"accuracy\"]}')\n",
    "print(f'Avg inference time: {results[\"avg_inference_time\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afca055a-564d-48de-bcca-dde66ec12d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Log: [{'answer': 'C', 'model_response': '<think>\\nFirst, the user is asking me to translate an English question into French based on the given instructions. The original English question isn\\'t directly stated, but from the context, it seems to be about the percentage of overweight adults in 2016. Looking at the answer options, A is 10%, B is 20%, C is 40%, D is 80%, but the question is about translation, not the actual answer.\\n\\nThe user says: \"The French equivalent must maintain the interrogative form while adjusting...\" and provides specific translations:\\n\\n- \"as of\" becomes \"en\"\\n\\n- \"about\" translates to \"environ\"\\n\\n- \"overweight\" is \"en surpoids\"\\n\\n- Age phrasing: \"âgés de 18 ans ou plus\" for \"aged 18 years or over\"\\n\\n- \"percentage\" becomes \"pourcentage\" without the % symbol since the original lacks it.\\n\\n- The question must end with a question mark.\\n\\n- The output must be clinically clean, with no introductory phrases or line breaks outside the tags.\\n\\nThe original English question isn\\'t explicitly given, but from the translation notes, it likely resembles something like: \"As of 2016, about what percentage of adults aged 18 years or over were overweight?\"\\n\\nBased on that, I need to construct the French equivalent.\\n\\nBreaking it down:\\n\\n- \"As of 2016\" → \"en 2016\" (since \"as of\" becomes \"en\")\\n\\n- \"about\" → \"environ\" (as specified)\\n\\n- \"what percentage\" → \"quel pourcentage\" (interrogative form)\\n\\n- \"of adults aged 18 years or over\" → \"des adultes âgés de 18 ans ou plus\" (as per instruction)\\n\\n- \"were overweight\" → \"étaient en surpoids\" (since \"overweight\" is \"en surpoids\")\\n\\n- End with a question mark.\\n\\nSo, the French question should be: \"En 2016, environ quel pourcentage des adultes âgés de 18 ans ou plus étaient en surpoids ?\"\\n\\nNow, I need to present this in the output format. The user said: \"Final output must be clinically clean - no introductory phrases or even line breaks outside the tags.\" And the tags are mentioned in the question: \"tags with pure output.\" Looking back, the user has \"<translation>\" in the response, so I should wrap the French question in <translation> tags.\\n\\nThe answer options are provided as A: 10 %, B: 20 %, C: 40 %, D: 80 %, but the user is asking for the translation of the question, not to answer it. The justification and choice should be for the translation, not the percentage. Re-reading the initial prompt: \"You are given a question in global facts with four answer options... You need to ponder the question and justify the choice of one of the options...\" But in this case, the \"question\" is about translation, not the fact itself. There might be a misunderstanding.\\n\\nThe user starts with: \"tags with pure output. The French equivalent must maintain...\" and then gives translation instructions. But the initial part says: \"You are given a question in global facts with four answer options...\" However, in the message, the question is implied to be about translation, and the answer options are percentages, which are for the factual question, not the translation.\\n\\nPerhaps the factual question is about the percentage of overweight adults, and I need to translate it into French as part of the response. But the user is asking me to justify choosing one of A, B, C, D, which are percentages.\\n\\nLet me look at the user\\'s message carefully:\\n\\n- \"QUESTION: tags with pure output.\" – This seems incomplete. Probably a typo or shorthand.\\n\\n- Then: \"The French equivalent must maintain the interrogative form while adjusting...\" – This is describing how to translate.\\n\\n- Then: \"Double-checking statistical terminology...\"\\n\\n- And finally, the answer options are given.\\n\\nBut in the answer options, B is \"tags. This is consistent...\" which looks messy. Option B says: \"B: tags. This is consistent with my role as a precise translation engine. ...\" That doesn\\'t make sense for a percentage. I think there might be an error in the user\\'s message.\\n\\nUpon closer inspection, in the user\\'s message, under \"ANSWER OPTIONS:\", it has:\\n\\nA: 10 %\\n\\nB: tags. This is consistent... (and it goes on with justification, but that seems out of place)\\n\\nPerhaps the user meant that the answer options are A, B, C, D with percentages, but B is incorrectly written. In the text, it says:\\n\\n\"ANSWER OPTIONS:\\n\\nA: 10 %\\n\\nB: tags. This is consistent with my role as a precise translation engine. \\n\\nI recall that percentage expressions in French use spaces around the percent sign, unlike English. So \"20%\" becomes \"20 %\" in French. \\n\\nThe user seems to value minimalism and precision, likely for some formal document or interface where extra text would be problematic. \\n\\nBetter double-check the spacing convention - yes, French typography requires a non-breaking space before the percent sign. \\n\\nSimple enough. Just need to output the exact French equivalent in the specified format.\"\\n\\nThis looks like it\\'s part of the justification or thinking process, not the answer option. I think the user pasted something incorrectly. The actual answer options should be:\\n\\nA: 10 %\\n\\nB: 20 %  (but it\\'s written as \"B: tags.\" which is confusing)\\n\\nIn the end, it says \"B: tags.\" but then has a paragraph. Perhaps it\\'s a mistake, and B is meant to be 20%, based on the context.\\n\\nLater it mentions \"Simple enough. Just need to output the exact French equivalent...\" which is part of the thinking.\\n\\nI think there\\'s a formatting issue. Re-reading the user\\'s entire message, it seems that the \"ANSWER OPTIONS\" section is mangled. Probably, the intended options are A: 10 %, B: 20 %, C: 40 %, D: 80 %, and the text after B is meant to be part of the thought process.\\n\\nMoreover, the user says: \"You need to ponder the question and justify the choice of one of the options A, B, C, or D.\" But the question is about translation, while the options are percentages for the factual question.\\n\\nPerhaps the \"question\" is the factual one about overweight percentage, and I need to translate it into French as instructed, but then also answer the factual question.\\n\\nThe user specifies: \"The French equivalent must...\" so the task is primarily to provide the translation.\\n\\nBut the initial prompt says: \"ponder the question and justify the choice of one of the options\", so I need to choose which percentage is correct.\\n\\nAs an AI expert in global facts, I should know or recall the correct statistic.\\n\\nThe question is: \"As of 2016, about what percentage of adults aged 18 years or over were overweight?\"\\n\\nFrom global health data, I remember that in 2016, according to WHO or other sources, a large portion of the global population was overweight or obese. Approximately 39% of adults aged 18 and over were overweight in 2016. But \"overweight\" typically includes both overweight and obese, and the percentage is high.\\n\\nUpon recollection:\\n\\n- In 2016, the WHO reported that more than 1.9 billion adults were overweight, of which over 650 million were obese.\\n\\n- The global adult population was around 4.5-5 billion at that time.\\n\\n- So, 1.9 billion overweight (including obese) out of about 4.8 billion adults would be roughly 40%.\\n\\nYes, about 39-40% of adults worldwide were overweight in 2016.\\n\\nLooking at the options:\\n\\nA: 10% – too low\\n\\nB: 20% – still low, but possible for some regions, not global\\n\\nC: 40% – matches my recollection\\n\\nD: 80% – too high; that might be for obesity in some countries, not global overweight.\\n\\nSo, C: 40% is the most accurate.\\n\\nBut the user is emphasizing the translation part. I need to provide the French translation as per instructions and also answer the factual question.\\n\\nThe user says: \"You need to ponder the question and justify the choice of one of the options A, B, C, or D.\" So I should choose the correct percentage.\\n\\nThen, at the end, output the chosen answer.\\n\\nBut the translation is part of the process. Perhaps I should first confirm the French equivalent.\\n\\nBased on the instructions, the French question is: \"En 2016, environ quel pourcentage des adultes âgés de 18 ans ou plus étaient en surpoids ?\"\\n\\nAnd it should be wrapped in <translation> tags, as per the user\\'s response format.\\n\\nIn the user\\'s message, they have \"<translation>\" in the response, so I should include that.\\n\\nFor the answer choice, I need to justify why 40% is correct.\\n\\nJustification: According to the World Health Organization (WHO), in 2016, approximately 39% of adults aged 18 years and older were overweight (including obese). This is based on global estimates. Option C is 40%, which is the closest to this figure. Options A and B are too low, and D is too high for a global average.\\n\\nNow, for the output, I need to provide the French translation in the specified format and then the answer choice.\\n\\nThe user says: \"Final output must be clinically clean - no introductory phrases or even line breaks outside the tags.\" So I should minimize extra text.\\n\\nAlso, \"at the end, do write the chosen answer option A, B, C, D after #ANSWER:\"\\n\\nSo in my response, I should have the translation, possibly with justification, and then #ANSWER: C or something.\\n\\nBut to be', 'is_correct': True, 'inference_time': 58.19391846656799}]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nEvaluation Log: {results[\"evaluation_log\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1cab11-4ba5-442e-9104-cba91ec3dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg_Inference_Time: 58.19secs\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAvg_Inference_Time: {results[\"avg_inference_time\"]:.2f}secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f361059-89e8-427e-ba20-fe4b1dee1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg_Inference_Time Method 2: 58.19secs\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAvg_Inference_Time Method 2: {results[\"avg_inference_time_Method2\"]:.2f}secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1faa77f-3003-4669-8a0d-e612f749f6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total_inference_time: 41.72472596168518\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = MMLUEvaluator(topic=\"global_facts\")\n",
    "inference_times = [] #Chong Ming. For Exercise task. to calculate average inference time, store inference_time in a list. Method 1\n",
    "total_inference_time = 0.0   #Chong Ming. For Exercise task. For purpose of collecting the time and derived an average. Method 2\n",
    "results = evaluator.run_evaluation(model=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "                         n_questions=1)\n",
    "\n",
    "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a006d6f-dec7-419f-93c6-013b895b221b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Log: [{'answer': 'C', 'model_response': '<think>\\nFirst, the question is: \"As of 2016, about what percentage of adults aged 18 years or older were overweight?\" The options are A: 10%, B: 20%, C: 40%, D: 80%.\\n\\nI need to recall global health statistics from around 2016. I remember that obesity and overweight are significant global health issues. The World Health Organization (WHO) often publishes data on this.\\n\\nI think the WHO reported that a large portion of the global population is overweight. Let me try to remember specific figures.\\n\\nIn 2016, the WHO might have had estimates. I recall that by 2016, more than 1.9 billion adults were overweight, and of those, over 650 million were obese. But I need the percentage, not the absolute number.\\n\\nTo find the percentage, I need the total adult population and the number who are overweight.\\n\\nAccording to WHO data, in 2016, approximately 39% of adults aged 18 years and older were overweight. That sounds familiar. I think that includes both overweight and obese individuals, but the question just says \"overweight,\" which might include obesity or might be separate. In common usage, \"overweight\" often includes those with a BMI of 25 or above, which encompasses both overweight and obese categories.\\n\\nLet me confirm the definition. WHO defines overweight as BMI ≥ 25, and obese as BMI ≥ 30. But when they report statistics, \"overweight\" sometimes refers to the broader category including obesity.\\n\\nLooking at the options: 10%, 20%, 40%, 80%. I think 40% is plausible.\\n\\nI remember a key report: the Global Burden of Disease study or WHO fact sheets.\\n\\nSpecifically, for 2016, WHO stated that 39% of adults aged 18 and over were overweight (BMI ≥ 25), and 13% were obese (BMI ≥ 30). So, overweight includes both.\\n\\nYes, that seems right. I think it was around 39-40%.\\n\\nOption C is 40%, which matches.\\n\\nBut let me make sure about the year. The question says \"as of 2016,\" so that should be fine.\\n\\nIs this global average? Yes, the question is about global facts, so it should be the worldwide percentage.\\n\\nOption D is 80%, which seems too high; that would mean almost everyone, which isn\\'t true. For example, in many developing countries, overweight rates are lower, while in developed countries, they are higher, but globally it averages out lower.\\n\\nOption A, 10%, is too low; I know that even in 2000, it was higher than that.\\n\\nOption B, 20%, also seems low. I think in the 1990s, it was around that, but by 2016, it had increased significantly.\\n\\nI recall that from WHO: in 2014, about 39% of adults were overweight, and by 2016, it was similar. Actually, the trend was increasing, but for 2016, it was around 39-40%.\\n\\nI can think about regional differences. For instance, in the US, overweight rates are very high, maybe 70% or more, but globally, including Africa and Asia, it balances to a lower number.\\n\\nYes, globally, it\\'s around 40%.\\n\\nAnother source: I think the NCD Risk Factor Collaboration or something. But WHO is authoritative.\\n\\nUpon second thought, I believe WHO reported that in 2016, 39% of adults aged 18 and over were overweight.\\n\\nConfirming: the WHO fact sheet on obesity from 2020 might have retrospective data, but for 2016.\\n\\nI can reason that in 2008, WHO said about 1.4 billion adults were overweight, which was about 35% at that time, and by 2016, it increased to around 39-40%.\\n\\nYes, that makes sense.\\n\\nOption C is 40%, which is close.\\n\\nBut let\\'s see the answer options; they don\\'t have 39%, so 40% is the closest.\\n\\nOption D is 80%, which is way off.\\n\\nI think I\\'m confident with 40%.\\n\\nBut just to be thorough, is there any chance it\\'s higher?\\n\\nFor example, some sources might report overweight including obesity separately, but no, the percentage given is usually the combined.\\n\\nPerhaps the question means only overweight, not obese? But the question says \"overweight,\" and in context, I think it includes the category of BMI ≥25.\\n\\nMoreover, in the options, 10% and 20% seem too low for the obese category alone, but the question is about overweight.\\n\\nObese alone was about 13% globally in 2016, I think.\\n\\nBut the question says \"overweight,\" which should include both.\\n\\nI think C:40% is correct.\\n\\nI recall that according to World Health Statistics 2017 or something, it was stated as 39%.\\n\\nBut since options are approximate, 40% is fine.\\n\\nPerhaps I should consider the age group: adults 18 or older, which is standard.\\n\\nAnother thing: is this for both sexes? Yes, globally.\\n\\nI think I\\'m overthinking it. Based on general knowledge, 40% is the right ballpark.\\n\\nSo, I\\'ll go with C:40%.\\n</think>\\nThe question asks for the percentage of adults aged 18 years or older who were overweight globally as of 2016. Overweight is defined by the World Health Organization (WHO) as having a body mass index (BMI) of 25 or higher, which includes the categories of overweight (BMI 25–29.9) and obese (BMI 30 or higher).\\n\\nAccording to WHO global health estimates and reports, such as the 2016 Global Health Observatory data, approximately 39% of adults aged 18 and over were overweight. This figure represents a global average, accounting for variations across regions due to factors like diet, lifestyle, and economic development. For instance:\\n- In high-income countries, rates were often higher (e.g., over 60% in some nations like the United States).\\n- In low- and middle-income countries, rates were lower but rising, contributing to the global average.\\n\\nNow, evaluating the answer options:\\n- A: 10% – This is too low, as even in 2000, the global rate was around 25-30%, and it increased by 2016.\\n- B: 20% – This is also too low, as rates in many regions exceeded 30% by 2010, and the global trend showed steady growth.\\n- C: 40% – This aligns closely with the WHO-reported 39% for 2016, making it the most accurate choice among the options.\\n- D: 80% – This is unrealistically high; while some countries had rates near 70%, the global average was significantly lower due to lower prevalence in regions like Africa and Asia.\\n\\nTherefore, based on authoritative global health data, 40% is the best estimate for 2016.\\n\\n#ANSWER: C', 'is_correct': True, 'inference_time': 41.72472596168518}]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nEvaluation Log: {results[\"evaluation_log\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae27884-980e-42d5-8e27-b12d02df8fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg_Inference_Time: 41.72secs\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAvg_Inference_Time: {results[\"avg_inference_time\"]:.2f}secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5f2f39f-7a8f-4825-b327-3592b7dedde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg_Inference_Time Method 2: 41.72secs\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAvg_Inference_Time Method 2: {results[\"avg_inference_time_Method2\"]:.2f}secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce6619-582b-4036-b503-a4970911191f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
